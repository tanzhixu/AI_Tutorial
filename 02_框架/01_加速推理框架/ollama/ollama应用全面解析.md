## 入门篇
### 1 ollama是什么？
Ollama 是一个支持在本地运行大语言模型的工具，兼容 Windows、Linux 和 MacOS 操作系统。使用 Ollama，您仅需一行命令即可启动模型。

### 2 如何安装？
Windows和MacOS用户，从下面链接下载安装即可：
下载地址：https://ollama.com/download

Linux系统安装请参考：
https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0036

### 3 第一次如何运行？
执行下面命令，会自动下载模型，并开始运行：
```
ollama run qwen
```
### 4 ollama支持哪些模型？
官方支持的模型，可以在 https://ollama.com/library 上面找到。

## 进阶篇
### 5 模型下载后存放到哪里了？
默认情况下，模型下载后存放在下面的位置：
```
  macOS: ~/.ollama/models
  Linux: /usr/share/ollama/.ollama/models #作为系统服务启动时
  Linux: /home/<username>/.ollama/models #当前用户启动时
  Windows: C:\Users\<username>\.ollama\models
```
### 6 如何修改下载模型的默认存放目录？
Windows用户
```
设置 OLLAMA_MODELS
# 只设置当前用户
setx OLLAMA_MODELS "D:\ollama_model" 
# 为所有用户设置
setx OLLAMA_MODELS "D:\ollama_model" /M
重启终端（setx命令在Windows中设置环境变量时，这个变量的更改只会在新打开的命令提示符窗口或终端会话中生效。）
重启ollama服务
```
Linux一般用户
```
# 打开下面文件
nano ~/.bashrc
# 添加设置
export OLLAMA_MODELS="/path/to/ollama_model"
重启终端
重启ollama服务： ollama serve
或者直接使用： OLLAMA_MODELS="/path/to/ollama_model" ollama serve 启动服务

Liunx root 服务模式
在服务文件中设置环境变量，并且要为新的目录设置ollama用户的读写权限

# 打开服务文件
sudo nano /etc/systemd/system/ollama.service
# 在文件中Service字段后添加
[Service]
Environment="OLLAMA_MODELS=/srv/models"
Environment="http_proxy=xxxxxx"

# 设置目录访问权限
sudo chown ollama:ollama /srv/models
sudo chmod u+w /srv/models

# 重启服务
sudo systemctl daemon-reload
sudo systemctl restart ollama.service

# 确认状态
sudo systemctl status ollama.service
```
### 7 ollama下载的模型与huggingface上的模型有什么区别？
通常情况下，Qwen 模型的表示方法为 Qwen1.5-4B-Chat。在 Ollama 中，Qwen 指代的是与 Hugging Face 上的 qwen1_5-4B-Chat-q4_0.gguf 模型相对应的版本，这是一个经过 4 位量化处理的模型。

ollama提供的qwen模型：默认为4B，4bit量化模型，大小为2.3G


Huggingface上的qwen1_5-4B-Chat-q4_0.gguf模型为：大小也为2.3G


### 8 什么是Modelfile？ 它的作用是什么？
在创建自定义模型时，需要一个配置文件来指定模型推理相关的设置。

这个文件仅在创建自定义模型过程中是必需的。若需修改模型推理的参数，必须重新创建模型，可以通过在 modelfile 中调整参数来实现。

### 9 自定义模型：如何 create 自定义模型（基于GGUF格式）？
制作自定义模型的过程如下（GGUF格式），以qwen1.5 0.5B模型为例：
```
下载模型 qwen1_5-0_5b-chat-q4_0.gguf
wget https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_0.gguf
准备modelfile文件
#FROM qwen1_5-0_5b-chat-q4_0.gguf
FROM ./qwen1_5-0_5b-chat-q4_0.gguf

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER repeat_penalty 1.05
PARAMETER top_k 20

TEMPLATE """{{ if and .First .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ .Response }}"""

# set the system message
SYSTEM """
You are a helpful assistant.
"""
创建模型
ollama create qwen0_5b -f Modelfile
```
### 10 自定义模型：模型创建后去了哪里？
模型被创建后，会被存放在以下位置：
模型文本被存储在： /home/<username>/.ollama/models/blobs
配置文件位于：/home/<username>/.ollama/models/manifests/registry.ollama.ai/library/qwen0_5b/latest

### 11 如何重新修改模型的 temperature 等参数？
模型被创建后，修改temperature等参数，需要重新create模型。

通过以下命令可以查看一个模型的 modelfile 设置
```
$ ollama show --modelfile qwen0_5b
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM qwen0_5b:latest

FROM /home/<username>/.ollama/models/blobs/sha256:46a9de8316739892e2721fdc49f8353155e4c1bcfa0b17867cb590d2dfdf1d99
TEMPLATE """{{ if and .First .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ .Response }}"""
SYSTEM """
You are a helpful assistant.
"""
PARAMETER repeat_penalty 1.05
PARAMETER temperature 0.7
PARAMETER top_k 20
PARAMETER top_p 0.8
可以看到这些数据都被存放到了 /home//.ollama/models/blobs/sha256:46a9de8316739892e2721fdc49f8353155e4c1bcfa0b17867cb590d2dfdf1d99 文件中（此文件是二进制文件）。通过重新create模型，可以修改里面的参数。
```
### 12 如何导入 PyTorch，Safetensors 格式的模型？
ollama只支持GGUF格式的模型进行导入。对于pytorch和safetensors的模型，需要转换为gguf格式之后再导入。
具体步骤，请参考：https://github.com/ollama/ollama/blob/main/docs/import.md

### 13 是否可以链接WebUI，有什么WebUI推荐？
ollama github首页中推荐了多种WebUI和终端访问方法的相关项目。
https://github.com/ollama/ollama （Community Integrations部分）

Linux安装教程请访问：
https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0036

### 14 如何对ollama进行速度评价？
```
当以普通用户身份启动服务时，可以在终端界面查看推理速度。比如
{"function":"print_timings","level":"INFO","line":257,"msg":"prompt eval time = 25.55 ms / 12 tokens ( 2.13 ms per token, 469.70 tokens per second)","n_prompt_tokens_processed":12,"n_tokens_second":469.70408642555196,"slot_id":0,"t_prompt_processing":25.548,"t_token":2.129,"task_id":111,"tid":"140543230871296","timestamp":1711096105}

可以看到有：2.13 ms per token, 469.70 tokens per second

在Windows或者Linux上以服务启动时，也可以在日志文件中找到这些数据。

# Windows
C:\Users\<username>\AppData\Local\Ollama\server.log
#MacOS
cat ~/.ollama/logs/server.log
# Linux
sudo systemctl status ollama.service > xxx.log
```
### 15 Ollama是否可以对模型精度评价？
在ollama工具中，没有直接的方式来评估模型性能。然而，在llama.cpp中，有提供测试数据集，以及使用Perplexity指标来进行模型评估的示例。

常见LLM大模型评估方法如下：

主观评价：通过人工审查模型的输出，评估其生成内容的质量和相关性。
测试集评价：利用特定的测试数据集，对模型性能进行量化评估。这种方法的细节可以参考相关模型的技术报告。
利用其他模型进行评价：例如，使用GPT-4对ollama模型的输出结果进行评估，以此来比较不同模型的性能。
### 16 Linux系统中以服务模式运行ollama，如何查看运行日志？
使用以下命令可以查看ollama的日志：
```
# 查看服务日志
sudo systemctl status ollama.service

# 查看用户日志
sudo journalctl -u ollama
```
### 17 Linux系统中如何卸载Ollama？
在Windows和MacOS上，卸载ollama的过程与卸载其他软件相同，支持一键卸载。

而在Linux上，卸载ollama需要执行更多步骤，包括关闭运行中的服务以及删除相关文件。具体操作步骤，请参考官方文档中的“Uninstall”部分：
https://github.com/ollama/ollama/blob/main/docs/linux.md

### 18 上网需要使用代理时，模型无法下载怎么办？
出现错误： Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/qwen/manifests/0.5b": dial tcp 34.120.132.20:443: i/o timeout

原因分析： 代理设置不正确。

服务文件中设置环境变量（sudo安装时）： 在以服务启动后，默认以ollama用户的身份运行。可以为ollama.service 服务设置环境变量。
```
# 打开服务文件
sudo nano /etc/systemd/system/ollama.service
# 在文件中Service字段后添加
[Service]
Environment="http_proxy=xxxxxx"

# 重启服务
sudo systemctl daemon-reload
sudo systemctl restart ollama.service

# 确认状态
sudo systemctl status ollama.service
以当前用户身份启动服务(一般用户)：
通过为当前用户设置代理，然后以当前用户的身份启动服务。这要求当前用户具有启动该服务所需的权限。
方法1：.bashrc中设置

# 在本地账户 .bashrc 文件中加入
export HTTP_PROXY=xxxxxxxxxxx
export HTTPS_PROXY=xxxxxxxxxxx

# 启动服务
ollama serve 启动（不能使用 sudo systemctl start ollama.service）
方法2：通过下面的方式启动服务

HTTPS_PROXY=xxxxxxxxxxx ollama serve
参考资料：https://github.com/ollama/ollama/issues/1859
```
### 19 有多个GPU时，如何指定使用单张GPU？
ollama默认会使用所有它可见的GPU，如果希望限制它只使用其中的某个GPU时，可以在环境变量中设置 CUDA_VISIBLE_DEVICES，比如： export CUDA_VISIBLE_DEVICES=0

### 20 ollama是否支持RAG？
ollama本身不支持RAG，但是可以结合langchain等工具来实现，比如：
https://github.com/ollama/ollama/tree/main/examples/langchain-python-rag-websummary
### 配置文件
我们将微调的参数统一放置在finetune_demo/configs下，将会有四个文件
.
├── deepspeed.json
├── lora.yaml
├── ptuning_v2.yaml
└── sft.yaml
分别为 deepspeed运行参数，lora配置文件，ptuningv2 配置文件和全参微调的配置文件

### 参数解释
每一份配置文件除了在细节上有一定区别，其他配置大同小异，我们针对每个参数做了简单的解释。
- data_config 这些参数是关于数据集的位置
  - train_file 训练集地址
  - val_file 验证集地址
  - test_file 测试集地址
- max_input_length微调模型输入的最大长度，默认为128。
- max_output_length 微调模型输出的最大长度，默认为256。
- peft_config Huggingface PEFT 框架的相关参数，peft_type 选择高效微调的方式，可以为LORA 或者 PREFIX_TUNING，并需要搭配对应的参数。
  - 如果你选择使用 LORA，则这些参数是必须的
    - LORA_RANK：这个参数决定了模型参数修改的复杂度和灵活性。较低的秩意味着更少的参数和更快的训练速度，但可能减少模型的灵活性。较高的秩可以提高模型的灵活性，但会增加参数数量和计算负担。我们默认设置为8。
    - lora_alpha是控制LoRA调整幅度的参数。它决定了对原始模型参数的修改程度。较高的lora_alpha值意味着对原始模型参数的更大调整，这可能有助于模型更好地适应新的任务或数据，但也可能导致过拟合。较低的值则意味着较小的调整，可能保持模型的泛化能力，但可能不足以充分适应新任务。我们默认设置为32
    - lora_dropout指的是在LoRA层应用的dropout比率。这意味着在训练过程中，网络的一部分连接会随机断开，以防止模型过度依赖于训练数据中的特定模式。较高的dropout比率可以增加模型的泛化能力，但也可能导致学习效率降低。我们设置为0.1。
    - target_modules（不可修改的参数）: 调整ChatGLM3系列模型必须使用query_key_value，这里没有在配置文件写出。
  - 如果你选择使用PREFIX_TUNING，则这些参数是必须的
    - num_virtual_tokens指定了在前缀调优中使用的虚拟令牌的数量。这里设置为 100，意味着在每个层前添加 100 个可训练的虚拟令牌。这些虚拟令牌对于引导模型学习特定任务是重要的，因为它们为模型提供了额外的、可调整的上下文信息。
> 如果你使用 sft 全量微调，则不应该有任何peft_config参数。
- training_args这些参数是关于训练中通用的参数
  - output_dir: 训练过程中输出文件的目录，默认是 ./output。
  - max_steps: 训练的最大步数，这里设置为 10000。
  - per_device_train_batch_size: 每个设备上的训练批量大小，默认是 4。
  - dataloader_num_workers: 数据加载时使用的工作进程数，默认值是 16。
  - remove_unused_columns: 是否移除未使用的列，默认设置为 False。
  -  save_strategy: 保存策略，默认是按步数保存（steps）。
    - save_steps: 保存步数，默认设置为每 500步保存一次。
    - log_level: 日志等级，默认设置为 info,可改为debug。
      - logging_steps: 日志记录步数，默认为每 10 步记录一次。
  - per_device_eval_batch_size: 每个设备上的评估批量大小，默认值为16。
  - evaluation_strategy: 评估策略，默认是按步数评估（steps）。
    - eval_steps: 评估步数，默认设置为每 500 步进行一次评估。
  -  predict_with_generate: 是否在预测时生成输出，这里设置为True。
  
#### lora.yaml
```
data_config:
  train_file: train.json
  val_file: dev.json
  test_file: dev.json
  num_proc: 16
max_input_length: 256
max_output_length: 512
training_args:
  output_dir: ./output
  max_steps: 3000
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  dataloader_num_workers: 16
  remove_unused_columns: false
  save_strategy: steps
  save_steps: 500
  log_level: info
  logging_strategy: steps
  logging_steps: 10
  per_device_eval_batch_size: 16
  evaluation_strategy: steps
  eval_steps: 500
  predict_with_generate: true
  generation_config:
    max_new_tokens: 512
  use_cpu: false
peft_config:
  peft_type: LORA
  task_type: CAUSAL_LM
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1
```

#### sft.yaml
```
data_config:
  train_file: train.json
  val_file: dev.json
  test_file: dev.json
  num_proc: 16
max_input_length: 256
max_output_length: 512
training_args:
  output_dir: ./output
  max_steps: 3000
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  dataloader_num_workers: 16
  remove_unused_columns: false
  save_strategy: steps
  save_steps: 500
  log_level: info
  logging_strategy: steps
  logging_steps: 10
  per_device_eval_batch_size: 16
  evaluation_strategy: steps
  eval_steps: 500
  predict_with_generate: true
  generation_config:
    max_new_tokens: 512
  deepspeed: ds_zero_3.json
```


#### ptuning_v2.yaml
```
data_config:
  train_file: train.json
  val_file: dev.json
  test_file: dev.json
  num_proc: 16
max_input_length: 256
max_output_length: 512
training_args:
  output_dir: ./output
  max_steps: 3000
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  dataloader_num_workers: 16
  remove_unused_columns: false
  save_strategy: steps
  save_steps: 500
  log_level: info
  logging_strategy: steps
  logging_steps: 10
  per_device_eval_batch_size: 16
  evaluation_strategy: steps
  eval_steps: 500
  predict_with_generate: true
  generation_config:
    max_new_tokens: 512
  use_cpu: false
peft_config:
  peft_type: PREFIX_TUNING
  task_type: CAUSAL_LM
  num_virtual_tokens: 128
```

#### ds_zero_2.json 
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },
    "bf16": {
        "enabled": "auto"
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 5e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

#### ds_zero_3.json 
```
{
  "train_micro_batch_size_per_gpu": "auto",
  "zero_allow_untested_optimizer": true,
  "bf16": {
    "enabled": "auto"
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": "auto",
      "betas": "auto",
      "eps": "auto",
      "weight_decay": "auto"
    }
  },
  "zero_optimization": {
    "stage": 3,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "reduce_scatter": true,
    "contiguous_gradients": true,
    "overlap_comm": true,
    "sub_group_size": 1e9,
    "reduce_bucket_size": "auto",
    "stage3_prefetch_bucket_size": "auto",
    "stage3_param_persistence_threshold": "auto",
    "stage3_max_live_parameters": 1e9,
    "stage3_max_reuse_distance": 1e9,
    "stage3_gather_16bit_weights_on_model_save": true
  }
}
```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:17:56.159400Z",
     "iopub.status.busy": "2024-04-19T07:17:56.159141Z",
     "iopub.status.idle": "2024-04-19T07:17:57.799967Z",
     "shell.execute_reply": "2024-04-19T07:17:57.799256Z",
     "shell.execute_reply.started": "2024-04-19T07:17:56.159375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken \n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:18:00.480377Z",
     "iopub.status.busy": "2024-04-19T07:18:00.479815Z",
     "iopub.status.idle": "2024-04-19T07:18:00.498954Z",
     "shell.execute_reply": "2024-04-19T07:18:00.498358Z",
     "shell.execute_reply.started": "2024-04-19T07:18:00.480344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2cf8b4e690>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_config_file(file_path):\n",
    "    # 创建 ConfigParser 对象\n",
    "    config = configparser.ConfigParser()\n",
    "    # 读取配置文件\n",
    "    config.read(file_path)\n",
    "    return config\n",
    "\n",
    "config_file_path = 'hyper-parameters.ini'\n",
    "config = read_config_file(config_file_path)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = config.getint('Hyperparameters', 'batch_size')\n",
    "context_length = config.getint('Hyperparameters', 'context_length')\n",
    "max_iters = config.getint('Hyperparameters', 'max_iters')\n",
    "learning_rate = config.getfloat('Hyperparameters', 'learning_rate')\n",
    "eval_interval = config.getint('Hyperparameters', 'eval_interval')\n",
    "eval_iters = config.getint('Hyperparameters', 'eval_iters')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T07:18:09.580787Z",
     "iopub.status.busy": "2024-04-19T07:18:09.580293Z",
     "iopub.status.idle": "2024-04-19T07:18:11.377096Z",
     "shell.execute_reply": "2024-04-19T07:18:11.376340Z",
     "shell.execute_reply.started": "2024-04-19T07:18:09.580754Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "with open('data/scifi.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T07:18:14.278943Z",
     "iopub.status.busy": "2024-04-19T07:18:14.278470Z",
     "iopub.status.idle": "2024-04-19T07:18:43.648360Z",
     "shell.execute_reply": "2024-04-19T07:18:43.647597Z",
     "shell.execute_reply.started": "2024-04-19T07:18:14.278912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = max_token_value = len(vocab)\n",
    "char2idx = {c:i for i,c in enumerate(vocab)}\n",
    "idx2char = {i:c for i,c in enumerate(vocab)}\n",
    "encode = lambda x: [char2idx[c] for c in x]\n",
    "decode = lambda idxs: ''.join([idx2char[i] for i in idxs])\n",
    "tokenized_text = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T07:20:13.687350Z",
     "iopub.status.busy": "2024-04-19T07:20:13.686886Z",
     "iopub.status.idle": "2024-04-19T07:20:13.691358Z",
     "shell.execute_reply": "2024-04-19T07:20:13.690691Z",
     "shell.execute_reply.started": "2024-04-19T07:20:13.687320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split train and validation set\n",
    "train_size = int(0.8 * len(tokenized_text))\n",
    "train_data = tokenized_text[:train_size]\n",
    "val_data = tokenized_text[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:20:15.483852Z",
     "iopub.status.busy": "2024-04-19T07:20:15.483377Z",
     "iopub.status.idle": "2024-04-19T07:20:19.817960Z",
     "shell.execute_reply": "2024-04-19T07:20:19.817184Z",
     "shell.execute_reply.started": "2024-04-19T07:20:15.483821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperparameters]\n",
      "context_length = 32\n",
      "d_model = 64\n",
      "num_blocks = 12\n",
      "num_heads = 8\n",
      "dropout = 0.1\n",
      "batch_size = 6\n",
      "learning_rate = 0.001\n",
      "max_iters = 5000\n",
      "eval_interval = 50\n",
      "eval_iters = 20\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "from model import Model\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:20:33.907519Z",
     "iopub.status.busy": "2024-04-19T07:20:33.907041Z",
     "iopub.status.idle": "2024-04-19T07:20:33.912802Z",
     "shell.execute_reply": "2024-04-19T07:20:33.911967Z",
     "shell.execute_reply.started": "2024-04-19T07:20:33.907485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get input embedding batch\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[idx:idx + context_length] for idx in idxs]).to(device)\n",
    "    y = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]).to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:20:36.043978Z",
     "iopub.status.busy": "2024-04-19T07:20:36.043493Z",
     "iopub.status.idle": "2024-04-19T07:20:36.049357Z",
     "shell.execute_reply": "2024-04-19T07:20:36.048718Z",
     "shell.execute_reply.started": "2024-04-19T07:20:36.043945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 32]) torch.Size([6, 32])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch('train')\n",
    "print(x_batch.shape, y_batch.shape )\n",
    "print(context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:21:05.596003Z",
     "iopub.status.busy": "2024-04-19T07:21:05.595500Z",
     "iopub.status.idle": "2024-04-19T07:21:05.600947Z",
     "shell.execute_reply": "2024-04-19T07:21:05.600204Z",
     "shell.execute_reply.started": "2024-04-19T07:21:05.595972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x_batch, y_batch = get_batch(split)\n",
    "            logits, loss = model(x_batch, y_batch)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T07:21:08.594320Z",
     "iopub.status.busy": "2024-04-19T07:21:08.593827Z",
     "iopub.status.idle": "2024-04-19T07:35:41.342365Z",
     "shell.execute_reply": "2024-04-19T07:35:41.341284Z",
     "shell.execute_reply.started": "2024-04-19T07:21:08.594286Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 7.402 Validation Loss: 7.458\n",
      "Step: 20 Training Loss: 6.904 Validation Loss: 7.02\n",
      "Step: 40 Training Loss: 6.933 Validation Loss: 6.832\n",
      "Step: 60 Training Loss: 6.536 Validation Loss: 6.719\n",
      "Step: 80 Training Loss: 6.672 Validation Loss: 6.571\n",
      "Step: 100 Training Loss: 6.576 Validation Loss: 6.512\n",
      "Step: 120 Training Loss: 6.285 Validation Loss: 6.387\n",
      "Step: 140 Training Loss: 6.321 Validation Loss: 6.431\n",
      "Step: 160 Training Loss: 6.348 Validation Loss: 6.436\n",
      "Step: 180 Training Loss: 6.349 Validation Loss: 6.253\n",
      "Step: 200 Training Loss: 6.169 Validation Loss: 6.32\n",
      "Step: 220 Training Loss: 6.196 Validation Loss: 6.29\n",
      "Step: 240 Training Loss: 6.237 Validation Loss: 6.198\n",
      "Step: 260 Training Loss: 6.215 Validation Loss: 6.264\n",
      "Step: 280 Training Loss: 6.142 Validation Loss: 6.219\n",
      "Step: 300 Training Loss: 6.042 Validation Loss: 6.13\n",
      "Step: 320 Training Loss: 6.133 Validation Loss: 6.135\n",
      "Step: 340 Training Loss: 6.083 Validation Loss: 6.114\n",
      "Step: 360 Training Loss: 6.014 Validation Loss: 6.05\n",
      "Step: 380 Training Loss: 6.039 Validation Loss: 6.013\n",
      "Step: 400 Training Loss: 6.025 Validation Loss: 6.038\n",
      "Step: 420 Training Loss: 6.056 Validation Loss: 5.905\n",
      "Step: 440 Training Loss: 5.922 Validation Loss: 6.016\n",
      "Step: 460 Training Loss: 5.984 Validation Loss: 5.952\n",
      "Step: 480 Training Loss: 5.896 Validation Loss: 5.971\n",
      "Step: 500 Training Loss: 5.928 Validation Loss: 5.997\n",
      "Step: 520 Training Loss: 5.813 Validation Loss: 5.877\n",
      "Step: 540 Training Loss: 5.818 Validation Loss: 5.889\n",
      "Step: 560 Training Loss: 5.892 Validation Loss: 5.767\n",
      "Step: 580 Training Loss: 5.823 Validation Loss: 5.757\n",
      "Step: 600 Training Loss: 5.782 Validation Loss: 5.828\n",
      "Step: 620 Training Loss: 5.888 Validation Loss: 5.946\n",
      "Step: 640 Training Loss: 5.742 Validation Loss: 5.773\n",
      "Step: 660 Training Loss: 5.839 Validation Loss: 5.793\n",
      "Step: 680 Training Loss: 5.708 Validation Loss: 5.817\n",
      "Step: 700 Training Loss: 5.76 Validation Loss: 5.857\n",
      "Step: 720 Training Loss: 5.771 Validation Loss: 5.822\n",
      "Step: 740 Training Loss: 5.753 Validation Loss: 5.774\n",
      "Step: 760 Training Loss: 5.667 Validation Loss: 5.842\n",
      "Step: 780 Training Loss: 5.723 Validation Loss: 5.802\n",
      "Step: 800 Training Loss: 5.749 Validation Loss: 5.671\n",
      "Step: 820 Training Loss: 5.805 Validation Loss: 5.715\n",
      "Step: 840 Training Loss: 5.641 Validation Loss: 5.685\n",
      "Step: 860 Training Loss: 5.659 Validation Loss: 5.685\n",
      "Step: 880 Training Loss: 5.553 Validation Loss: 5.714\n",
      "Step: 900 Training Loss: 5.664 Validation Loss: 5.641\n",
      "Step: 920 Training Loss: 5.745 Validation Loss: 5.641\n",
      "Step: 940 Training Loss: 5.681 Validation Loss: 5.605\n",
      "Step: 960 Training Loss: 5.596 Validation Loss: 5.554\n",
      "Step: 980 Training Loss: 5.574 Validation Loss: 5.617\n",
      "Step: 1000 Training Loss: 5.613 Validation Loss: 5.627\n",
      "Step: 1020 Training Loss: 5.594 Validation Loss: 5.483\n",
      "Step: 1040 Training Loss: 5.573 Validation Loss: 5.717\n",
      "Step: 1060 Training Loss: 5.441 Validation Loss: 5.549\n",
      "Step: 1080 Training Loss: 5.469 Validation Loss: 5.639\n",
      "Step: 1100 Training Loss: 5.62 Validation Loss: 5.462\n",
      "Step: 1120 Training Loss: 5.488 Validation Loss: 5.417\n",
      "Step: 1140 Training Loss: 5.515 Validation Loss: 5.521\n",
      "Step: 1160 Training Loss: 5.426 Validation Loss: 5.467\n",
      "Step: 1180 Training Loss: 5.494 Validation Loss: 5.525\n",
      "Step: 1200 Training Loss: 5.445 Validation Loss: 5.524\n",
      "Step: 1220 Training Loss: 5.453 Validation Loss: 5.569\n",
      "Step: 1240 Training Loss: 5.463 Validation Loss: 5.461\n",
      "Step: 1260 Training Loss: 5.561 Validation Loss: 5.464\n",
      "Step: 1280 Training Loss: 5.46 Validation Loss: 5.469\n",
      "Step: 1300 Training Loss: 5.461 Validation Loss: 5.484\n",
      "Step: 1320 Training Loss: 5.479 Validation Loss: 5.497\n",
      "Step: 1340 Training Loss: 5.509 Validation Loss: 5.414\n",
      "Step: 1360 Training Loss: 5.398 Validation Loss: 5.547\n",
      "Step: 1380 Training Loss: 5.403 Validation Loss: 5.516\n",
      "Step: 1400 Training Loss: 5.487 Validation Loss: 5.403\n",
      "Step: 1420 Training Loss: 5.452 Validation Loss: 5.432\n",
      "Step: 1440 Training Loss: 5.548 Validation Loss: 5.455\n",
      "Step: 1460 Training Loss: 5.362 Validation Loss: 5.495\n",
      "Step: 1480 Training Loss: 5.386 Validation Loss: 5.375\n",
      "Step: 1500 Training Loss: 5.405 Validation Loss: 5.336\n",
      "Step: 1520 Training Loss: 5.33 Validation Loss: 5.362\n",
      "Step: 1540 Training Loss: 5.371 Validation Loss: 5.458\n",
      "Step: 1560 Training Loss: 5.412 Validation Loss: 5.515\n",
      "Step: 1580 Training Loss: 5.382 Validation Loss: 5.338\n",
      "Step: 1600 Training Loss: 5.339 Validation Loss: 5.335\n",
      "Step: 1620 Training Loss: 5.373 Validation Loss: 5.448\n",
      "Step: 1640 Training Loss: 5.24 Validation Loss: 5.391\n",
      "Step: 1660 Training Loss: 5.291 Validation Loss: 5.22\n",
      "Step: 1680 Training Loss: 5.271 Validation Loss: 5.266\n",
      "Step: 1700 Training Loss: 5.354 Validation Loss: 5.399\n",
      "Step: 1720 Training Loss: 5.459 Validation Loss: 5.466\n",
      "Step: 1740 Training Loss: 5.395 Validation Loss: 5.296\n",
      "Step: 1760 Training Loss: 5.329 Validation Loss: 5.342\n",
      "Step: 1780 Training Loss: 5.326 Validation Loss: 5.305\n",
      "Step: 1800 Training Loss: 5.254 Validation Loss: 5.397\n",
      "Step: 1820 Training Loss: 5.363 Validation Loss: 5.263\n",
      "Step: 1840 Training Loss: 5.278 Validation Loss: 5.285\n",
      "Step: 1860 Training Loss: 5.325 Validation Loss: 5.254\n",
      "Step: 1880 Training Loss: 5.231 Validation Loss: 5.416\n",
      "Step: 1900 Training Loss: 5.432 Validation Loss: 5.434\n",
      "Step: 1920 Training Loss: 5.3 Validation Loss: 5.383\n",
      "Step: 1940 Training Loss: 5.207 Validation Loss: 5.246\n",
      "Step: 1960 Training Loss: 5.324 Validation Loss: 5.202\n",
      "Step: 1980 Training Loss: 5.285 Validation Loss: 5.339\n",
      "Step: 2000 Training Loss: 5.351 Validation Loss: 5.286\n",
      "Step: 2020 Training Loss: 5.244 Validation Loss: 5.271\n",
      "Step: 2040 Training Loss: 5.168 Validation Loss: 5.282\n",
      "Step: 2060 Training Loss: 5.314 Validation Loss: 5.301\n",
      "Step: 2080 Training Loss: 5.26 Validation Loss: 5.365\n",
      "Step: 2100 Training Loss: 5.245 Validation Loss: 5.222\n",
      "Step: 2120 Training Loss: 5.338 Validation Loss: 5.235\n",
      "Step: 2140 Training Loss: 5.235 Validation Loss: 5.242\n",
      "Step: 2160 Training Loss: 5.316 Validation Loss: 5.326\n",
      "Step: 2180 Training Loss: 5.202 Validation Loss: 5.235\n",
      "Step: 2200 Training Loss: 5.149 Validation Loss: 5.359\n",
      "Step: 2220 Training Loss: 5.251 Validation Loss: 5.246\n",
      "Step: 2240 Training Loss: 5.361 Validation Loss: 5.208\n",
      "Step: 2260 Training Loss: 5.213 Validation Loss: 5.215\n",
      "Step: 2280 Training Loss: 5.259 Validation Loss: 5.374\n",
      "Step: 2300 Training Loss: 5.262 Validation Loss: 5.172\n",
      "Step: 2320 Training Loss: 5.221 Validation Loss: 5.276\n",
      "Step: 2340 Training Loss: 5.2 Validation Loss: 5.185\n",
      "Step: 2360 Training Loss: 5.205 Validation Loss: 5.073\n",
      "Step: 2380 Training Loss: 5.147 Validation Loss: 5.2\n",
      "Step: 2400 Training Loss: 5.158 Validation Loss: 5.115\n",
      "Step: 2420 Training Loss: 5.221 Validation Loss: 5.18\n",
      "Step: 2440 Training Loss: 5.202 Validation Loss: 5.193\n",
      "Step: 2460 Training Loss: 5.21 Validation Loss: 5.274\n",
      "Step: 2480 Training Loss: 5.181 Validation Loss: 5.215\n",
      "Step: 2500 Training Loss: 5.2 Validation Loss: 5.257\n",
      "Step: 2520 Training Loss: 5.188 Validation Loss: 5.209\n",
      "Step: 2540 Training Loss: 5.01 Validation Loss: 5.38\n",
      "Step: 2560 Training Loss: 5.149 Validation Loss: 5.136\n",
      "Step: 2580 Training Loss: 5.123 Validation Loss: 5.228\n",
      "Step: 2600 Training Loss: 5.246 Validation Loss: 5.262\n",
      "Step: 2620 Training Loss: 5.116 Validation Loss: 5.173\n",
      "Step: 2640 Training Loss: 5.248 Validation Loss: 5.271\n",
      "Step: 2660 Training Loss: 5.046 Validation Loss: 5.05\n",
      "Step: 2680 Training Loss: 5.073 Validation Loss: 5.273\n",
      "Step: 2700 Training Loss: 5.12 Validation Loss: 5.201\n",
      "Step: 2720 Training Loss: 5.132 Validation Loss: 5.168\n",
      "Step: 2740 Training Loss: 5.072 Validation Loss: 5.154\n",
      "Step: 2760 Training Loss: 5.078 Validation Loss: 5.049\n",
      "Step: 2780 Training Loss: 5.225 Validation Loss: 5.21\n",
      "Step: 2800 Training Loss: 5.136 Validation Loss: 5.175\n",
      "Step: 2820 Training Loss: 5.175 Validation Loss: 5.193\n",
      "Step: 2840 Training Loss: 5.088 Validation Loss: 5.188\n",
      "Step: 2860 Training Loss: 5.254 Validation Loss: 5.2\n",
      "Step: 2880 Training Loss: 5.164 Validation Loss: 5.174\n",
      "Step: 2900 Training Loss: 5.148 Validation Loss: 5.142\n",
      "Step: 2920 Training Loss: 5.093 Validation Loss: 5.036\n",
      "Step: 2940 Training Loss: 5.128 Validation Loss: 5.127\n",
      "Step: 2960 Training Loss: 5.082 Validation Loss: 5.114\n",
      "Step: 2980 Training Loss: 5.165 Validation Loss: 5.259\n",
      "Step: 3000 Training Loss: 5.093 Validation Loss: 5.125\n",
      "Step: 3020 Training Loss: 5.283 Validation Loss: 5.155\n",
      "Step: 3040 Training Loss: 5.152 Validation Loss: 5.125\n",
      "Step: 3060 Training Loss: 5.114 Validation Loss: 5.112\n",
      "Step: 3080 Training Loss: 5.222 Validation Loss: 5.131\n",
      "Step: 3100 Training Loss: 5.052 Validation Loss: 5.022\n",
      "Step: 3120 Training Loss: 5.116 Validation Loss: 5.165\n",
      "Step: 3140 Training Loss: 5.131 Validation Loss: 5.087\n",
      "Step: 3160 Training Loss: 5.187 Validation Loss: 5.087\n",
      "Step: 3180 Training Loss: 5.009 Validation Loss: 5.061\n",
      "Step: 3200 Training Loss: 5.171 Validation Loss: 5.09\n",
      "Step: 3220 Training Loss: 5.105 Validation Loss: 5.114\n",
      "Step: 3240 Training Loss: 5.155 Validation Loss: 5.164\n",
      "Step: 3260 Training Loss: 5.139 Validation Loss: 5.025\n",
      "Step: 3280 Training Loss: 5.094 Validation Loss: 5.12\n",
      "Step: 3300 Training Loss: 5.111 Validation Loss: 5.096\n",
      "Step: 3320 Training Loss: 5.082 Validation Loss: 4.991\n",
      "Step: 3340 Training Loss: 5.098 Validation Loss: 4.987\n",
      "Step: 3360 Training Loss: 5.138 Validation Loss: 5.165\n",
      "Step: 3380 Training Loss: 5.095 Validation Loss: 5.101\n",
      "Step: 3400 Training Loss: 5.037 Validation Loss: 5.164\n",
      "Step: 3420 Training Loss: 4.986 Validation Loss: 5.111\n",
      "Step: 3440 Training Loss: 5.083 Validation Loss: 5.16\n",
      "Step: 3460 Training Loss: 5.055 Validation Loss: 5.082\n",
      "Step: 3480 Training Loss: 5.006 Validation Loss: 5.103\n",
      "Step: 3500 Training Loss: 4.908 Validation Loss: 5.138\n",
      "Step: 3520 Training Loss: 5.083 Validation Loss: 5.182\n",
      "Step: 3540 Training Loss: 5.013 Validation Loss: 5.072\n",
      "Step: 3560 Training Loss: 4.977 Validation Loss: 5.125\n",
      "Step: 3580 Training Loss: 4.968 Validation Loss: 4.968\n",
      "Step: 3600 Training Loss: 5.107 Validation Loss: 5.089\n",
      "Step: 3620 Training Loss: 5.066 Validation Loss: 5.072\n",
      "Step: 3640 Training Loss: 4.885 Validation Loss: 5.112\n",
      "Step: 3660 Training Loss: 5.123 Validation Loss: 5.16\n",
      "Step: 3680 Training Loss: 5.044 Validation Loss: 5.162\n",
      "Step: 3700 Training Loss: 5.042 Validation Loss: 5.06\n",
      "Step: 3720 Training Loss: 4.993 Validation Loss: 5.129\n",
      "Step: 3740 Training Loss: 5.165 Validation Loss: 5.073\n",
      "Step: 3760 Training Loss: 5.074 Validation Loss: 5.003\n",
      "Step: 3780 Training Loss: 5.003 Validation Loss: 5.122\n",
      "Step: 3800 Training Loss: 5.039 Validation Loss: 5.188\n",
      "Step: 3820 Training Loss: 5.067 Validation Loss: 5.044\n",
      "Step: 3840 Training Loss: 4.986 Validation Loss: 5.003\n",
      "Step: 3860 Training Loss: 5.004 Validation Loss: 4.892\n",
      "Step: 3880 Training Loss: 4.994 Validation Loss: 4.948\n",
      "Step: 3900 Training Loss: 4.979 Validation Loss: 5.081\n",
      "Step: 3920 Training Loss: 5.019 Validation Loss: 5.111\n",
      "Step: 3940 Training Loss: 5.106 Validation Loss: 5.005\n",
      "Step: 3960 Training Loss: 4.973 Validation Loss: 5.107\n",
      "Step: 3980 Training Loss: 5.068 Validation Loss: 5.075\n",
      "Step: 4000 Training Loss: 4.972 Validation Loss: 5.109\n",
      "Step: 4020 Training Loss: 4.877 Validation Loss: 4.94\n",
      "Step: 4040 Training Loss: 5.011 Validation Loss: 5.033\n",
      "Step: 4060 Training Loss: 4.981 Validation Loss: 5.018\n",
      "Step: 4080 Training Loss: 5.117 Validation Loss: 5.116\n",
      "Step: 4100 Training Loss: 4.99 Validation Loss: 4.959\n",
      "Step: 4120 Training Loss: 4.966 Validation Loss: 5.054\n",
      "Step: 4140 Training Loss: 4.852 Validation Loss: 5.124\n",
      "Step: 4160 Training Loss: 5.042 Validation Loss: 4.972\n",
      "Step: 4180 Training Loss: 4.992 Validation Loss: 4.937\n",
      "Step: 4200 Training Loss: 5.05 Validation Loss: 5.037\n",
      "Step: 4220 Training Loss: 5.052 Validation Loss: 4.933\n",
      "Step: 4240 Training Loss: 5.016 Validation Loss: 5.103\n",
      "Step: 4260 Training Loss: 4.958 Validation Loss: 5.105\n",
      "Step: 4280 Training Loss: 4.925 Validation Loss: 5.034\n",
      "Step: 4300 Training Loss: 5.058 Validation Loss: 4.962\n",
      "Step: 4320 Training Loss: 5.012 Validation Loss: 4.961\n",
      "Step: 4340 Training Loss: 4.918 Validation Loss: 4.995\n",
      "Step: 4360 Training Loss: 4.922 Validation Loss: 4.872\n",
      "Step: 4380 Training Loss: 5.078 Validation Loss: 5.035\n",
      "Step: 4400 Training Loss: 4.996 Validation Loss: 5.032\n",
      "Step: 4420 Training Loss: 4.895 Validation Loss: 5.041\n",
      "Step: 4440 Training Loss: 5.038 Validation Loss: 4.914\n",
      "Step: 4460 Training Loss: 5.114 Validation Loss: 5.008\n",
      "Step: 4480 Training Loss: 4.923 Validation Loss: 4.944\n",
      "Step: 4500 Training Loss: 4.974 Validation Loss: 5.027\n",
      "Step: 4520 Training Loss: 4.941 Validation Loss: 5.003\n",
      "Step: 4540 Training Loss: 4.965 Validation Loss: 4.954\n",
      "Step: 4560 Training Loss: 4.991 Validation Loss: 5.025\n",
      "Step: 4580 Training Loss: 4.935 Validation Loss: 4.833\n",
      "Step: 4600 Training Loss: 4.89 Validation Loss: 5.059\n",
      "Step: 4620 Training Loss: 4.927 Validation Loss: 4.976\n",
      "Step: 4640 Training Loss: 4.859 Validation Loss: 5.018\n",
      "Step: 4660 Training Loss: 4.996 Validation Loss: 4.92\n",
      "Step: 4680 Training Loss: 5.05 Validation Loss: 5.018\n",
      "Step: 4700 Training Loss: 4.958 Validation Loss: 4.873\n",
      "Step: 4720 Training Loss: 4.853 Validation Loss: 4.958\n",
      "Step: 4740 Training Loss: 4.864 Validation Loss: 5.126\n",
      "Step: 4760 Training Loss: 4.942 Validation Loss: 4.985\n",
      "Step: 4780 Training Loss: 4.904 Validation Loss: 4.824\n",
      "Step: 4800 Training Loss: 5.077 Validation Loss: 4.938\n",
      "Step: 4820 Training Loss: 4.896 Validation Loss: 4.913\n",
      "Step: 4840 Training Loss: 4.792 Validation Loss: 5.113\n",
      "Step: 4860 Training Loss: 4.907 Validation Loss: 4.931\n",
      "Step: 4880 Training Loss: 4.913 Validation Loss: 4.941\n",
      "Step: 4900 Training Loss: 4.951 Validation Loss: 5.055\n",
      "Step: 4920 Training Loss: 4.864 Validation Loss: 5.026\n",
      "Step: 4940 Training Loss: 5.005 Validation Loss: 5.06\n",
      "Step: 4960 Training Loss: 4.839 Validation Loss: 4.888\n",
      "Step: 4980 Training Loss: 4.882 Validation Loss: 4.988\n",
      "Step: 4999 Training Loss: 4.934 Validation Loss: 4.775\n"
     ]
    }
   ],
   "source": [
    "# Use AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "tracked_losses = list()\n",
    "for step in range(max_iters):\n",
    "    if step % eval_iters == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        tracked_losses.append(losses)\n",
    "        print('Step:', step, 'Training Loss:', round(losses['train'].item(), 3), 'Validation Loss:',\n",
    "              round(losses['valid'].item(), 3))\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:04:15.090087Z",
     "iopub.status.busy": "2024-04-18T09:04:15.089602Z",
     "iopub.status.idle": "2024-04-18T09:04:15.227113Z",
     "shell.execute_reply": "2024-04-18T09:04:15.226340Z",
     "shell.execute_reply.started": "2024-04-18T09:04:15.090057Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'model/model-ckpt.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

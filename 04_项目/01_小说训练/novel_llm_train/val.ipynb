{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:10:34.713528Z",
     "iopub.status.busy": "2024-04-18T09:10:34.712937Z",
     "iopub.status.idle": "2024-04-18T09:10:36.417131Z",
     "shell.execute_reply": "2024-04-18T09:10:36.416405Z",
     "shell.execute_reply.started": "2024-04-18T09:10:34.713491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Model \n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:10:37.973508Z",
     "iopub.status.busy": "2024-04-18T09:10:37.972598Z",
     "iopub.status.idle": "2024-04-18T09:10:37.982726Z",
     "shell.execute_reply": "2024-04-18T09:10:37.982059Z",
     "shell.execute_reply.started": "2024-04-18T09:10:37.973475Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f87f4b466b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_config_file(file_path):\n",
    "    # 创建 ConfigParser 对象\n",
    "    config = configparser.ConfigParser()\n",
    "    # 读取配置文件\n",
    "    config.read(file_path)\n",
    "    return config\n",
    "\n",
    "config_file_path = 'hyper-parameters.ini'\n",
    "config = read_config_file(config_file_path)\n",
    "\n",
    "batch_size = config.getint('Hyperparameters', 'batch_size')\n",
    "context_length = config.getint('Hyperparameters', 'batch_size')\n",
    "d_model = config.getint('Hyperparameters', 'd_model')\n",
    "num_blocks = config.getint('Hyperparameters', 'num_blocks')\n",
    "num_heads = config.getint('Hyperparameters', 'num_heads')\n",
    "learning_rate = config.getfloat('Hyperparameters', 'learning_rate')\n",
    "dropout = config.getfloat('Hyperparameters', 'dropout')\n",
    "max_iters = config.getint('Hyperparameters', 'max_iters')\n",
    "eval_interval = config.getint('Hyperparameters', 'eval_interval')\n",
    "eval_iters = config.getint('Hyperparameters', 'eval_iters')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if it's available.\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:10:40.073689Z",
     "iopub.status.busy": "2024-04-18T09:10:40.073220Z",
     "iopub.status.idle": "2024-04-18T09:10:41.871996Z",
     "shell.execute_reply": "2024-04-18T09:10:41.871257Z",
     "shell.execute_reply.started": "2024-04-18T09:10:40.073659Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/scifi.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:10:53.322431Z",
     "iopub.status.busy": "2024-04-18T09:10:53.321536Z",
     "iopub.status.idle": "2024-04-18T09:11:21.843114Z",
     "shell.execute_reply": "2024-04-18T09:11:21.842323Z",
     "shell.execute_reply.started": "2024-04-18T09:10:53.322387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = max_token_value = len(vocab)\n",
    "char2idx = {c:i for i,c in enumerate(vocab)}\n",
    "idx2char = {i:c for i,c in enumerate(vocab)}\n",
    "encode = lambda x: [char2idx[c] for c in x]\n",
    "decode = lambda idxs: ''.join([idx2char[i] for i in idxs])\n",
    "tokenized_text = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T09:12:34.134107Z",
     "iopub.status.busy": "2024-04-18T09:12:34.133611Z",
     "iopub.status.idle": "2024-04-18T09:12:34.308256Z",
     "shell.execute_reply": "2024-04-18T09:12:34.307491Z",
     "shell.execute_reply.started": "2024-04-18T09:12:34.134070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:12:44.901429Z",
     "iopub.status.busy": "2024-04-18T09:12:44.900928Z",
     "iopub.status.idle": "2024-04-18T09:12:45.014125Z",
     "shell.execute_reply": "2024-04-18T09:12:45.013328Z",
     "shell.execute_reply.started": "2024-04-18T09:12:44.901388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (token_embedding_lookup_table): Embedding(100001, 64)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (multi_head_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Attention(\n",
       "            (key_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (query_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (value_layer): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (12): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (language_model_out_linear_layer): Linear(in_features=64, out_features=100000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/model-ckpt.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T09:13:55.272150Z",
     "iopub.status.busy": "2024-04-18T09:13:55.271661Z",
     "iopub.status.idle": "2024-04-18T09:13:55.275849Z",
     "shell.execute_reply": "2024-04-18T09:13:55.275135Z",
     "shell.execute_reply.started": "2024-04-18T09:13:55.272117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = '奥特曼出生在一个小村庄'\n",
    "start_ids = encode(start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:14:06.138415Z",
     "iopub.status.busy": "2024-04-18T09:14:06.137918Z",
     "iopub.status.idle": "2024-04-18T09:14:06.143123Z",
     "shell.execute_reply": "2024-04-18T09:14:06.142349Z",
     "shell.execute_reply.started": "2024-04-18T09:14:06.138375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_776/2442113015.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = (torch.tensor(torch.tensor(start_ids,dtype=torch.long,device=device)[None,...]))\n"
     ]
    }
   ],
   "source": [
    "x = (torch.tensor(torch.tensor(start_ids,dtype=torch.long,device=device)[None,...]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T09:14:13.728984Z",
     "iopub.status.busy": "2024-04-18T09:14:13.728494Z",
     "iopub.status.idle": "2024-04-18T09:14:17.607909Z",
     "shell.execute_reply": "2024-04-18T09:14:17.607233Z",
     "shell.execute_reply.started": "2024-04-18T09:14:13.728941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "奥特曼出生在一个小村庄，紧们不必要醒，她问，无荷纳也不是安全失罗？一次在一条小树，种月闪齐恰队属，似乎吃发出了一颗绍禁适栗有漆。”在那种两十二件，而且，没见回答：“你们也没有发生。”\n",
      "但是大再度地提她的光线散已在一个维底球\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "y = model.generate(x)\n",
    "print('-----------------')\n",
    "print(decode(y[0].tolist()))\n",
    "print('-----------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
